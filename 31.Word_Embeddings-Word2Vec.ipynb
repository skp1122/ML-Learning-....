{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model\n",
    "- Word2Vec Google's Pretrained Model\n",
    "- Contains vector representations of 50 billion words\n",
    "- Words which are similar in context have similar vectors\n",
    "- Distance/Similarity between two words can be measured using Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "- Text Similarity\n",
    "- Language Translation\n",
    "- Finding Odd Words\n",
    "- Word Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "- Word embeddings are numerical representation of words, in the form of vectors.\n",
    "- Word2Vec Model represents each word as 300 Dimensional Vector\n",
    "- In this tutorial we are going to see how to use pre-trained word2vec model.\n",
    "- Model size is around 1.5 GB\n",
    "- We will work using Gensim, which is popular NLP Package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Genism's Word2Vec Model provides optimum implementation of:- \n",
    "- 1) CBOW Model\n",
    "- 2) SkipGram Model\n",
    "- Paper 1 -> Efficient Estimation of Word Representations in Vector Space\n",
    "- Paper 2 -> Distributed Representations of Words and Phrases and their Compositionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec using Gensim\n",
    "- Link https://radimarehurek.com/gesim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE\n",
    "### Load Word2Vec Model\n",
    "**KeyedVectors** -> This object essentially contains the mapping between words and embedings. After training, it can be used directly to query those embeddings in variou ways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('.\\dataset\\GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "v_apple = word_vectors[\"apple\"]\n",
    "v_mango = word_vectors[\"mango\"]\n",
    "\n",
    "print(v_apple.shape)\n",
    "print(v_mango.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57518554]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([v_apple],[v_mango]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - Answering - Find the Odd One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_one_out(words):\n",
    "    \"\"\"Accepts a list of words and returns the odd one\"\"\"\n",
    "    # Generate all word embedings in the given list\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\n",
    "    \n",
    "    #print(len(all_word_vectors))\n",
    "    #print(all_word_vectors)\n",
    "    avg_vector = np.mean(all_word_vectors,axis=0)\n",
    "    #print(avg_vector.shape)\n",
    "    \n",
    "    # Iterate over every word and find similarity\n",
    "    odd_one = None\n",
    "    min_similarity = 1.0  # very high value\n",
    "    \n",
    "    for w in words:\n",
    "        sim = cosine_similarity([word_vectors[w]],[avg_vector])\n",
    "        if sim < min_similarity:\n",
    "            min_similarity = sim\n",
    "            odd_one = w\n",
    "            \n",
    "        print(\"similarity between %s and avg vector is %.2f\"%(w,sim))\n",
    "    return odd_one\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between apple and avg vector is 0.78\n",
      "similarity between mango and avg vector is 0.76\n",
      "similarity between juice and avg vector is 0.71\n",
      "similarity between party and avg vector is 0.36\n",
      "similarity between orange and avg vector is 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'party'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between music and avg vector is 0.66\n",
      "similarity between dance and avg vector is 0.81\n",
      "similarity between sleep and avg vector is 0.51\n",
      "similarity between dancer and avg vector is 0.72\n",
      "similarity between food and avg vector is 0.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between match and avg vector is 0.58\n",
      "similarity between player and avg vector is 0.68\n",
      "similarity between football and avg vector is 0.72\n",
      "similarity between cricket and avg vector is 0.70\n",
      "similarity between dancer and avg vector is 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dancer'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between india and avg vector is 0.81\n",
      "similarity between paris and avg vector is 0.75\n",
      "similarity between russia and avg vector is 0.79\n",
      "similarity between france and avg vector is 0.81\n",
      "similarity between germany and avg vector is 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"]\n",
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]\n",
    "input_3 = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\n",
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies Task\n",
    "- In the word analogy , we complete the sentence \"a is to b as c is to __ \". An example is \"man is to woman as king is to Queen\". In detail , we are trying to find a word d, such that the associated word vectors ea,eb,ec,ed are related in the following manner: eb-ea=ed-ec. We will measure the similarity between eb-ea and ed-ec using cosine similarity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "man -> woman :: prince -> Princes\n",
    "italy -> italian :: spain -> spanish\n",
    "india -> delhi :: japan -> tokyo\n",
    "man -> woman :: boy -> girl\n",
    "small -> smaller :: large -> larger\n",
    "#### Try it out\n",
    "man -> coder :: woman -> ___ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32617188,  0.13085938,  0.03466797, -0.08300781,  0.08984375,\n",
       "       -0.04125977, -0.19824219,  0.00689697,  0.14355469,  0.0019455 ,\n",
       "        0.02880859, -0.25      , -0.08398438, -0.15136719, -0.10205078,\n",
       "        0.04077148, -0.09765625,  0.05932617,  0.02978516, -0.10058594,\n",
       "       -0.13085938,  0.001297  ,  0.02612305, -0.27148438,  0.06396484,\n",
       "       -0.19140625, -0.078125  ,  0.25976562,  0.375     , -0.04541016,\n",
       "        0.16210938,  0.13671875, -0.06396484, -0.02062988, -0.09667969,\n",
       "        0.25390625,  0.24804688, -0.12695312,  0.07177734,  0.3203125 ,\n",
       "        0.03149414, -0.03857422,  0.21191406, -0.00811768,  0.22265625,\n",
       "       -0.13476562, -0.07617188,  0.01049805, -0.05175781,  0.03808594,\n",
       "       -0.13378906,  0.125     ,  0.0559082 , -0.18261719,  0.08154297,\n",
       "       -0.08447266, -0.07763672, -0.04345703,  0.08105469, -0.01092529,\n",
       "        0.17480469,  0.30664062, -0.04321289, -0.01416016,  0.09082031,\n",
       "       -0.00927734, -0.03442383, -0.11523438,  0.12451172, -0.0246582 ,\n",
       "        0.08544922,  0.14355469, -0.27734375,  0.03662109, -0.11035156,\n",
       "        0.13085938, -0.01721191, -0.08056641, -0.00708008, -0.02954102,\n",
       "        0.30078125, -0.09033203,  0.03149414, -0.18652344, -0.11181641,\n",
       "        0.10253906, -0.25976562, -0.02209473,  0.16796875, -0.05322266,\n",
       "       -0.14550781, -0.01049805, -0.03039551, -0.03857422,  0.11523438,\n",
       "       -0.0062561 , -0.13964844,  0.08007812,  0.06103516, -0.15332031,\n",
       "       -0.11132812, -0.14160156,  0.19824219, -0.06933594,  0.29296875,\n",
       "       -0.16015625,  0.20898438,  0.00041771,  0.01831055, -0.20214844,\n",
       "        0.04760742,  0.05810547, -0.0123291 , -0.01989746, -0.00364685,\n",
       "       -0.0135498 , -0.08251953, -0.03149414,  0.00717163,  0.20117188,\n",
       "        0.08300781, -0.0480957 , -0.26367188, -0.09667969, -0.22558594,\n",
       "       -0.09667969,  0.06494141, -0.02502441,  0.08496094,  0.03198242,\n",
       "       -0.07568359, -0.25390625, -0.11669922, -0.01446533, -0.16015625,\n",
       "       -0.00701904, -0.05712891,  0.02807617, -0.09179688,  0.25195312,\n",
       "        0.24121094,  0.06640625,  0.12988281,  0.17089844, -0.13671875,\n",
       "        0.1875    , -0.10009766, -0.04199219, -0.12011719,  0.00524902,\n",
       "        0.15625   , -0.203125  , -0.07128906, -0.06103516,  0.01635742,\n",
       "        0.18261719,  0.03588867, -0.04248047,  0.16796875, -0.15039062,\n",
       "       -0.16992188,  0.01831055,  0.27734375, -0.01269531, -0.0390625 ,\n",
       "       -0.15429688,  0.18457031, -0.07910156,  0.09033203, -0.02709961,\n",
       "        0.08251953,  0.06738281, -0.16113281, -0.19628906, -0.15234375,\n",
       "       -0.04711914,  0.04760742,  0.05908203, -0.16894531, -0.14941406,\n",
       "        0.12988281,  0.04321289,  0.02624512, -0.1796875 , -0.19628906,\n",
       "        0.06445312,  0.08935547,  0.1640625 , -0.03808594, -0.09814453,\n",
       "       -0.01483154,  0.1875    ,  0.12792969,  0.22753906,  0.01818848,\n",
       "       -0.07958984, -0.11376953, -0.06933594, -0.15527344, -0.08105469,\n",
       "       -0.09277344, -0.11328125, -0.15136719, -0.08007812, -0.05126953,\n",
       "       -0.15332031,  0.11669922,  0.06835938,  0.0324707 , -0.33984375,\n",
       "       -0.08154297, -0.08349609,  0.04003906,  0.04907227, -0.24121094,\n",
       "       -0.13476562, -0.05932617,  0.12158203, -0.34179688,  0.16503906,\n",
       "        0.06176758, -0.18164062,  0.20117188, -0.07714844,  0.1640625 ,\n",
       "        0.00402832,  0.30273438, -0.10009766, -0.13671875, -0.05957031,\n",
       "        0.0625    , -0.21289062, -0.06542969,  0.1796875 , -0.07763672,\n",
       "       -0.01928711, -0.15039062, -0.00106049,  0.03417969,  0.03344727,\n",
       "        0.19335938,  0.01965332, -0.19921875, -0.10644531,  0.01525879,\n",
       "        0.00927734,  0.01416016, -0.02392578,  0.05883789,  0.02368164,\n",
       "        0.125     ,  0.04760742, -0.05566406,  0.11572266,  0.14746094,\n",
       "        0.1015625 , -0.07128906, -0.07714844, -0.12597656,  0.0291748 ,\n",
       "        0.09521484, -0.12402344, -0.109375  , -0.12890625,  0.16308594,\n",
       "        0.28320312, -0.03149414,  0.12304688, -0.23242188, -0.09375   ,\n",
       "       -0.12988281,  0.0135498 , -0.03881836, -0.08251953,  0.00897217,\n",
       "        0.16308594,  0.10546875, -0.13867188, -0.16503906, -0.03857422,\n",
       "        0.10839844, -0.10498047,  0.06396484,  0.38867188, -0.05981445,\n",
       "       -0.0612793 , -0.10449219, -0.16796875,  0.07177734,  0.13964844,\n",
       "        0.15527344, -0.03125   , -0.20214844, -0.12988281, -0.10058594,\n",
       "       -0.06396484, -0.08349609, -0.30273438, -0.08007812,  0.02099609],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[\"man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(a,b,c,word_vectors):\n",
    "    \"\"\"Accepts a traid of  words,  a,b,c and returns d such that a is to b : c is to d\"\"\"\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    \n",
    "    # similarity |b-a| = |c-d| should be max\n",
    "    max_similarity = -100\n",
    "    \n",
    "    d = None\n",
    "    \n",
    "    words = word_vectors.vocab.keys()\n",
    "    \n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    \n",
    "    # to find d such that similarity(|b-a|,|d-c|) should be max\n",
    "    \n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "            \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triad_2 = (\"man\",\"woman\",\"prince\")\n",
    "predict_word(*triad_2,word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Most Similar Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['woman','king'],negative=['man'],topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun Project (Bollywood Matching pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Data Preparation\n",
    "- Each sentence must be tokenized, into a list of words.\n",
    "- The sentence can be text loaded into memory once, or we can build a data pipeline which iteratively feeds data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bollywood.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7d31db329eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bollywood.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7d31db329eea>\u001b[0m in \u001b[0;36mreadFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bollywood.txt'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopw = set(stopwords.words('english'))\n",
    "\n",
    "def readFile(file):\n",
    "    f = open(file,'r',encoding='utf-8')\n",
    "    text = f.read()\n",
    "    \n",
    "    # Tokenization - sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    print(len(sentences))\n",
    "    \n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stopw]\n",
    "        data.append(words)\n",
    "        \n",
    "    return data\n",
    "\n",
    "text = readFile(\"bollywood.txt\")\n",
    "print(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c90fc095e93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(text,size=300,window=10,min_count=1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-867158da5fe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(vocab)\n",
    "print(model[\"deepika\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_actor(a,b,c,word_vectors):\n",
    "    \"\"\"Accepts a triad of words and return d such that a is to b : c is to d\"\"\"\n",
    "    \n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100\n",
    "    \n",
    "    d = None\n",
    "    \n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    options = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\n",
    "    \n",
    "    for w in options:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "            \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim>max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    \n",
    "    return d\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30d34319298b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtriad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"nick\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"priyanka\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"virat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtriad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "triad = (\"nick\",\"priyanka\",\"virat\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
